{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yuekang/anaconda3/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN =pd.read_csv(\"TransitionMatrix1.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TE =pd.read_csv(\"TransitionMatrix2.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS =pd.read_csv(\"TransitionMatrix3.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TW = pd.read_csv(\"TransitionMatrix4.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tn = TN.values.reshape(1,14,14)\n",
    "Te = TE.values.reshape(1,14,14)\n",
    "Ts = TS.values.reshape(1,14,14)\n",
    "Tw = TW.values.reshape(1,14,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.concatenate((Tn\n",
    "               ,Te,Ts,Tw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 14, 14)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reward = pd.read_csv(\"RewardMatrix.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reward_matrix = np.zeros((4,14,14))\n",
    "for i in range(4):\n",
    "    for j in range(14):\n",
    "        for k in range(14):\n",
    "            Reward_matrix[i,j,k] = Reward.values[j,k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyEvaluation(Policy, T, R, Absorbing, gamma, tol):\n",
    "    #from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    newV = np.zeros(S)\n",
    "    V = np.zeros(S)\n",
    "    Delta = 2*tol\n",
    "    diffVec = np.zeros(S)\n",
    "    while Delta > tol:\n",
    "        for priorState in range(S):\n",
    "            if Absorbing[priorState]:\n",
    "                continue\n",
    "            else:\n",
    "                tmpV = 0\n",
    "                for action in range(A):\n",
    "                    tmpQ = 0\n",
    "                    for postState in range(S):\n",
    "                        tmpQ = tmpQ + T[action,postState,priorState]*(R[action,postState,priorState]+ gamma*V[postState])\n",
    "                    tmpV = tmpV + Policy[action]*tmpQ\n",
    "                newV[priorState] = tmpV\n",
    "            newV[priorState] = tmpV\n",
    "            diffVec[priorState] = np.abs(newV[priorState] - V[priorState])\n",
    "            V[priorState] = newV[priorState]\n",
    "        Delta = np.max(diffVec)\n",
    "    return V\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    S = 14\n",
    "    StateNames = ['s01','s02','s03','s04','s05','s06','s07','s08','s09','s10','s11','s12','s13','s14']\n",
    "    A = 4\n",
    "    ActionNames = ['N','E','S','W',];\n",
    "    Absorbing = [0,1,1,0,0,0,0,0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = PolicyEvaluation([0.25,0.25,0.25,0.25],T,Reward_matrix,Absorbing,0.5,0.00001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.31028699,  0.        ,  0.        , -4.79235224, -1.86173076,\n",
       "       -1.74554618, -4.35709451, -2.75411779, -1.97629207, -2.12938149,\n",
       "       -1.99603812, -1.99995214, -2.0036869 , -2.02217744])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(poststate,priorstate):\n",
    "    Policy = [0.25,0.25,0.25,0.25]\n",
    "    p = 0\n",
    "    for i in range(4):\n",
    "        p = p + Policy[i]*T[i,poststate,priorstate]\n",
    "    return p\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood(2,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009765625"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.25*likelihood(9,13)*likelihood(7,9)*likelihood(3,7)*likelihood(2,3)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "for i,j in enumerate(StateNames):\n",
    "    dic[j] = np.array([StateNames,\n",
    "                      Tn[0,:,i],Te[0,:,i],Ts[0,:,i],Tw[0,:,i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Markov(object):\n",
    "\n",
    "    def __init__(self,state_dict):\n",
    "        self.state_dict = state_dict\n",
    "        self.state = list(self.state_dict.keys())[0]\n",
    "\n",
    "    def check_state(self):\n",
    "        print('Current State: %s' % (self.state))\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.state = state\n",
    "        #print('State is now: %s' % (self.state))\n",
    "\n",
    "    def next_state(self,i):\n",
    "        A = self.state_dict[self.state]\n",
    "        self.state = np.random.choice(a=list(A[0]), p=list(A[i]))\n",
    "        #print('New State: %s' % (self.state))\n",
    "\n",
    "dic = {}\n",
    "for i,j in enumerate(StateNames):\n",
    "    dic[j] = np.array([StateNames,\n",
    "                      Tn[0,:,i],Te[0,:,i],Ts[0,:,i],Tw[0,:,i]])\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_trace_old(dic):\n",
    "#     diagram_a = Markov(dic)\n",
    "    \n",
    "#     trace=[]\n",
    "    \n",
    "    \n",
    "#     diagram_a.set_state(np.random.choice(a=['s11','s12','s13','s14'],p=[0.25,0.25,0.25,0.25]))\n",
    "#     priorState = StateNames.index(diagram_a.state)\n",
    "#     trace.append(diagram_a.state)\n",
    "    \n",
    "    \n",
    "#     #print(diagram_a.state,end=',')\n",
    "#     action_sample = np.random.choice(a=['N','E','S','W'],p=[0.25,0.25,0.25,0.25])\n",
    "#     #print(action_sample,end=',')\n",
    "#     trace.append(action_sample)\n",
    "#     action_sample_index = ActionNames.index(action_sample)+1\n",
    "    \n",
    "#     diagram_a.next_state(action_sample_index)\n",
    "#     postState = StateNames.index(diagram_a.state)\n",
    "#     #print(diagram_a.state,end=',')\n",
    "    \n",
    "#     trace.append(Reward_matrix[action_sample_index-1,postState,priorState])\n",
    "    \n",
    "#     #trace.append(diagram_a.state)\n",
    "    \n",
    "    \n",
    "\n",
    "#     while (diagram_a.state != 's02') and (diagram_a.state != 's03'):   #Reward_matrix[action_sample_index-1,postState,priorState] != -10 :\n",
    "#         priorState = StateNames.index(diagram_a.state)\n",
    "#         #print(diagram_a.state,end=',')\n",
    "#         trace.append(diagram_a.state)\n",
    "#         action_sample = np.random.choice(a=['N','E','S','W'],p=[0.25,0.25,0.25,0.25])\n",
    "#         #print(action_sample,end=',')\n",
    "#         trace.append(action_sample)\n",
    "#         action_sample_index = ActionNames.index(action_sample)+1\n",
    "#         diagram_a.next_state(action_sample_index)\n",
    "#         diagram_a.set_state(diagram_a.state)\n",
    "#         postState = StateNames.index(diagram_a.state) \n",
    "#         #print(diagram_a.state,end=',')\n",
    "#         trace.append(Reward_matrix[action_sample_index-1,postState,priorState])\n",
    "#         #trace.append(diagram_a.state)\n",
    "#     print('end')\n",
    "#     return trace\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trace(dic,policy):\n",
    "    diagram_a = Markov(dic)\n",
    "    \n",
    "    trace=[]\n",
    "    \n",
    "    \n",
    "    diagram_a.set_state(np.random.choice(a=['s11','s12','s13','s14'],p=[0.25,0.25,0.25,0.25]))\n",
    "#     priorState = StateNames.index(diagram_a.state)\n",
    "#     trace.append(diagram_a.state)\n",
    "    \n",
    "    \n",
    "#     #print(diagram_a.state,end=',')\n",
    "#     action_sample = policy[diagram_a.state]\n",
    "#     #print(action_sample,end=',')\n",
    "#     trace.append(action_sample)\n",
    "#     action_sample_index = ActionNames.index(action_sample)+1\n",
    "    \n",
    "#     diagram_a.next_state(action_sample_index)\n",
    "#     diagram_a.set_state(diagram_a.state)\n",
    "#     postState = StateNames.index(diagram_a.state)\n",
    "#     #print(diagram_a.state,end=',')\n",
    "    \n",
    "#     trace.append(Reward_matrix[action_sample_index-1,postState,priorState])\n",
    "    \n",
    "#     #trace.append(diagram_a.state)\n",
    "    \n",
    "    \n",
    "\n",
    "    while (diagram_a.state != 's02') and (diagram_a.state != 's03'):   #Reward_matrix[action_sample_index-1,postState,priorState] != -10 :\n",
    "        priorState = StateNames.index(diagram_a.state)\n",
    "        print(diagram_a.state,end=',')\n",
    "        trace.append(diagram_a.state)\n",
    "        action_sample = policy[diagram_a.state]\n",
    "        #print(action_sample,end=',')\n",
    "        trace.append(action_sample)\n",
    "        action_sample_index = ActionNames.index(action_sample)+1\n",
    "        diagram_a.next_state(action_sample_index)\n",
    "        #diagram_a.set_state(diagram_a.state)\n",
    "        postState = StateNames.index(diagram_a.state) \n",
    "        print(diagram_a.state,end=',')\n",
    "        trace.append(Reward_matrix[action_sample_index-1,postState,priorState])\n",
    "        #trace.append(diagram_a.state)\n",
    "    print('\\nend')    \n",
    "    return trace\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s11': 'N', 's13': 'W', 's03': 'E', 's09': 'N', 's12': 'S', 's07': 'W', 's04': 'W', 's08': 'E', 's14': 'W', 's10': 'S', 's01': 'E', 's06': 'N', 's02': 'E', 's05': 'S'}\n",
      "s13,s12,s12,s12,s12,s13,s13,s12,s12,s11,s11,s09,s09,s05,s05,s05,s05,s09,s09,s05,s05,s09,s09,s09,s09,s05,s05,s09,s09,s09,s09,s09,s09,s09,s09,s09,s09,s05,s05,s06,s06,s02,\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "policy = {}\n",
    "for s in StateNames:\n",
    "    policy[s] = np.random.choice(ActionNames)\n",
    "print(policy)\n",
    "    \n",
    "x = get_trace(dic,policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_estimation(x):\n",
    "    G = 0\n",
    "    GAMMA = 0.5\n",
    "    states_actions_returns = []\n",
    "    for i in range(len(x)-1,-1,-3):\n",
    "        states_actions_returns.append((x[i-2],x[i-1], G))\n",
    "        G = x[i] + GAMMA*G\n",
    "    states_actions_returns.reverse()\n",
    "    return states_actions_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s13', 'W', -1.9999961853027344),\n",
       " ('s12', 'S', -1.9999923706054688),\n",
       " ('s12', 'S', -1.9999847412109375),\n",
       " ('s13', 'W', -1.999969482421875),\n",
       " ('s12', 'S', -1.99993896484375),\n",
       " ('s11', 'N', -1.9998779296875),\n",
       " ('s09', 'N', -1.999755859375),\n",
       " ('s05', 'S', -1.99951171875),\n",
       " ('s05', 'S', -1.9990234375),\n",
       " ('s09', 'N', -1.998046875),\n",
       " ('s05', 'S', -1.99609375),\n",
       " ('s09', 'N', -1.9921875),\n",
       " ('s09', 'N', -1.984375),\n",
       " ('s05', 'S', -1.96875),\n",
       " ('s09', 'N', -1.9375),\n",
       " ('s09', 'N', -1.875),\n",
       " ('s09', 'N', -1.75),\n",
       " ('s09', 'N', -1.5),\n",
       " ('s09', 'N', -1.0),\n",
       " ('s05', 'S', 0.0),\n",
       " ('s06', 'N', 0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = monte_carlo_estimation(x)\n",
    "trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(dic):\n",
    "    policy = {}\n",
    "    for s in StateNames:\n",
    "        policy[s] = np.random.choice(ActionNames)\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    for s in StateNames:\n",
    "        Q[s] = {}\n",
    "        for a in ActionNames:\n",
    "            Q[s][a] = 0\n",
    "            returns[(s,a)] = []\n",
    "    deltas = []\n",
    "    for t in range(1000):\n",
    "        biggest_change = 0\n",
    "        x = get_trace(dic,policy)\n",
    "        states_actions_returns = monte_carlo_estimation(x)\n",
    "        seen_state_action_pairs = set()\n",
    "        for s, a, G in states_actions_returns:\n",
    "            sa = (s, a)\n",
    "            if sa not in seen_state_action_pairs:\n",
    "                returns[sa].append(G)\n",
    "                old_q = Q[s][a]\n",
    "        # the new Q[s][a] is the sample mean of all our returns for that (state, action)\n",
    "                Q[s][a] = np.mean(returns[sa])\n",
    "                biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "                seen_state_action_pairs.add(sa)\n",
    "        deltas.append(biggest_change)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def monte_carlo_estimation_old(S,dic,number_of_sequsence):\n",
    "#     newV = np.zeros(S)\n",
    "#     Return =np.zeros(S)\n",
    "#     for i in range(number_of_sequsence):\n",
    "#         x = get_trace(dic)\n",
    "#         reward_trace = [x[i] for i in range(2,len(x),3)]\n",
    "#         R=[]\n",
    "#         gamma = 0.5\n",
    "#         for name in StateNames:\n",
    "#             try:\n",
    "#                 first_to_appear = x.index(name)/3\n",
    "#                 r= 0\n",
    "#                 for i in range(int(first_to_appear),len(reward_trace)):\n",
    "#                     r = r+ gamma**(i-int(first_to_appear))*reward_trace[i]\n",
    "#                 R.append(r)\n",
    "#             except:\n",
    "#                 R.append(0)\n",
    "\n",
    "#         Return = Return+ R\n",
    "#     return Return/number_of_sequsence\n",
    "\n",
    "# r = monte_carlo_estimation(S,dic,1000)\n",
    "# print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(d):\n",
    "  # returns the argmax (key) and max (value) from a dictionary\n",
    "  # put this into a function since we are using it so often\n",
    "    max_key = None\n",
    "    max_val = float('-inf')\n",
    "    for k, v in d.items():\n",
    "        if v > max_val:\n",
    "            max_val = v\n",
    "            max_key = k\n",
    "    return max_key, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_a_element(ActionNames,a):\n",
    "    act = ActionNames[:]\n",
    "    del act[act.index(a)]\n",
    "    return act\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_monte_carlo(dic,eps=0.1):\n",
    "    policy = {}\n",
    "    for s in StateNames:\n",
    "        policy[s] = np.random.choice(ActionNames)\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    for s in StateNames:\n",
    "        Q[s] = {}\n",
    "        for a in ActionNames:\n",
    "            Q[s][a] = 0\n",
    "            returns[(s,a)] = []\n",
    "    deltas = []\n",
    "    for t in range(2):\n",
    "        if t % 1 == 0:\n",
    "            print(t)\n",
    "        biggest_change = 0\n",
    "        x = get_trace(dic,policy)\n",
    "        states_actions_returns = monte_carlo_estimation(x)\n",
    "        seen_state_action_pairs = set()\n",
    "        for s, a, G in states_actions_returns:\n",
    "            sa = (s, a)\n",
    "            if sa not in seen_state_action_pairs:\n",
    "                returns[sa].append(G)\n",
    "                old_q = Q[s][a]\n",
    "        # the new Q[s][a] is the sample mean of all our returns for that (state, action)\n",
    "                Q[s][a] = np.mean(returns[sa])\n",
    "                biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "                seen_state_action_pairs.add(sa)\n",
    "        deltas.append(biggest_change)\n",
    "        for s in policy.keys():\n",
    "            a, _ = max_dict(Q[s])\n",
    "            policy[s] = np.random.choice([a]+delete_a_element(ActionNames,a),p=[(1-eps+eps/4),eps/4,eps/4,eps/4])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
